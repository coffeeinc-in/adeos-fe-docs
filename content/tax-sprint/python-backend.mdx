# Python Backend Implementation

## Python Backend API with FastAPI and Redis

### Main Application (app/main.py)

```python
from fastapi import FastAPI, HTTPException, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn
import asyncio
from typing import List, Optional
import json
import uuid
from datetime import datetime
import os
import logging

from .services.redis_service import RedisService
from .services.queue_service import QueueService
from .services.extraction_service import ExtractionService
from .models.schemas import JobResponse, JobStatus, JobStatusResponse
from .routes.api import router as api_router

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DocumentProcessingAPI:
    def __init__(self):
        self.app = FastAPI(
            title="Document Processing API",
            description="API for document processing with WebSocket real-time updates",
            version="1.0.0"
        )
        
        self.setup_middleware()
        self.setup_routes()
        
        # Initialize services
        self.redis_service = None
        self.queue_service = None
        self.extraction_service = None

    def setup_middleware(self):
        # CORS middleware
        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=os.getenv("ALLOWED_ORIGINS", "http://localhost:3000").split(","),
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )

        # Add custom middleware for request logging
        @self.app.middleware("http")
        async def log_requests(request, call_next):
            start_time = datetime.now()
            response = await call_next(request)
            process_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"{request.method} {request.url.path} - {response.status_code} - {process_time:.3f}s")
            return response

    def setup_routes(self):
        # Include API routes
        self.app.include_router(api_router, prefix="/api")
        
        # Health check endpoint
        @self.app.get("/health")
        async def health_check():
            redis_status = "healthy" if await self.redis_service.ping() else "unhealthy"
            queue_size = await self.queue_service.get_queue_size()
            
            return {
                "status": "healthy",
                "redis": redis_status,
                "queue_size": queue_size,
                "timestamp": datetime.now().isoformat()
            }

        # Root endpoint
        @self.app.get("/")
        async def root():
            return {
                "message": "Document Processing API",
                "version": "1.0.0",
                "docs": "/docs"
            }

    async def startup(self):
        """Initialize services on startup"""
        try:
            # Initialize Redis connection
            self.redis_service = RedisService()
            await self.redis_service.connect()
            
            # Initialize queue service
            self.queue_service = QueueService(self.redis_service)
            
            # Initialize extraction service
            self.extraction_service = ExtractionService(self.redis_service, self.queue_service)
            
            # Start background job processor
            asyncio.create_task(self.queue_service.start_job_processor())
            
            logger.info("All services initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize services: {e}")
            raise

    async def shutdown(self):
        """Cleanup on shutdown"""
        try:
            if self.queue_service:
                await self.queue_service.stop_job_processor()
            
            if self.redis_service:
                await self.redis_service.disconnect()
                
            logger.info("Services shut down successfully")
            
        except Exception as e:
            logger.error(f"Error during shutdown: {e}")

# Create FastAPI app instance
app_instance = DocumentProcessingAPI()
app = app_instance.app

# Add startup and shutdown events
@app.on_event("startup")
async def startup_event():
    await app_instance.startup()

@app.on_event("shutdown")
async def shutdown_event():
    await app_instance.shutdown()

# Make services available to routes
app.state.redis_service = lambda: app_instance.redis_service
app.state.queue_service = lambda: app_instance.queue_service
app.state.extraction_service = lambda: app_instance.extraction_service

if __name__ == "__main__":
    uvicorn.run(
        "app.main:app",
        host="0.0.0.0",
        port=int(os.getenv("PORT", 8000)),
        reload=os.getenv("ENVIRONMENT") == "development",
        log_level="info"
    )
```

### Data Models (app/models/schemas.py)

```python
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any, Union
from datetime import datetime
from enum import Enum

class JobStatusEnum(str, Enum):
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

class ExtractionType(str, Enum):
    TEXT = "text"
    TABLE = "table"
    DIAGRAM = "diagram"

class TextExtractionResult(BaseModel):
    content: str
    metadata: Dict[str, Any] = Field(default_factory=dict)

class TableData(BaseModel):
    data: List[List[str]]
    headers: List[str]
    position: Dict[str, Any] = Field(default_factory=dict)

class TableExtractionResult(BaseModel):
    tables: List[TableData]

class ImageData(BaseModel):
    url: str
    description: str
    type: str
    position: Dict[str, Any] = Field(default_factory=dict)

class DiagramExtractionResult(BaseModel):
    images: List[ImageData]

class ExtractionResults(BaseModel):
    text_extraction: Optional[TextExtractionResult] = None
    table_extraction: Optional[TableExtractionResult] = None
    diagram_extraction: Optional[DiagramExtractionResult] = None

class JobCreate(BaseModel):
    extraction_types: List[ExtractionType] = Field(default=[ExtractionType.TEXT])
    options: Dict[str, Any] = Field(default_factory=dict)

class JobStatus(BaseModel):
    job_id: str
    status: JobStatusEnum
    progress: float = 0.0
    current_task: str = ""
    results: Optional[ExtractionResults] = None
    error: Optional[str] = None
    created_at: datetime
    updated_at: datetime
    file_name: Optional[str] = None
    file_size: Optional[int] = None

class JobResponse(BaseModel):
    job_id: str
    status: JobStatusEnum
    message: str
    created_at: datetime

class JobStatusResponse(BaseModel):
    job: JobStatus
    
class ErrorResponse(BaseModel):
    error: str
    detail: Optional[str] = None
    job_id: Optional[str] = None

class ProcessingUpdate(BaseModel):
    job_id: str
    progress: float
    current_task: str
    status: JobStatusEnum
    data: Optional[Dict[str, Any]] = None
    timestamp: datetime = Field(default_factory=datetime.now)

# File upload models
class FileUploadResponse(BaseModel):
    filename: str
    size: int
    content_type: str
    upload_path: str

# Progress tracking models
class ProgressUpdate(BaseModel):
    stage: str
    progress: float
    message: str
    details: Optional[Dict[str, Any]] = None

# Queue models
class QueuedJob(BaseModel):
    job_id: str
    file_path: str
    extraction_types: List[ExtractionType]
    options: Dict[str, Any]
    created_at: datetime
    priority: int = 0

class QueueStatus(BaseModel):
    pending_jobs: int
    processing_jobs: int
    completed_jobs: int
    failed_jobs: int
    queue_depth: int
```

### Redis Service (app/services/redis_service.py)

```python
import redis.asyncio as redis
import json
import os
import logging
from typing import Any, Optional, Dict, List
import asyncio

logger = logging.getLogger(__name__)

class RedisService:
    def __init__(self):
        self.redis_client = None
        self.pubsub = None
        self.config = {
            'host': os.getenv('REDIS_HOST', 'localhost'),
            'port': int(os.getenv('REDIS_PORT', 6379)),
            'password': os.getenv('REDIS_PASSWORD'),
            'db': int(os.getenv('REDIS_DB', 0)),
            'decode_responses': True,
            'socket_keepalive': True,
            'socket_keepalive_options': {},
            'health_check_interval': 30
        }

    async def connect(self):
        """Establish Redis connection"""
        try:
            self.redis_client = redis.Redis(**self.config)
            await self.redis_client.ping()
            logger.info("Connected to Redis successfully")
        except Exception as e:
            logger.error(f"Failed to connect to Redis: {e}")
            raise

    async def disconnect(self):
        """Close Redis connection"""
        try:
            if self.pubsub:
                await self.pubsub.close()
            if self.redis_client:
                await self.redis_client.close()
            logger.info("Disconnected from Redis")
        except Exception as e:
            logger.error(f"Error disconnecting from Redis: {e}")

    async def ping(self) -> bool:
        """Check Redis connection health"""
        try:
            await self.redis_client.ping()
            return True
        except Exception:
            return False

    # Basic Redis operations
    async def get(self, key: str) -> Optional[str]:
        """Get value by key"""
        try:
            return await self.redis_client.get(key)
        except Exception as e:
            logger.error(f"Error getting key {key}: {e}")
            return None

    async def set(self, key: str, value: Any, ex: Optional[int] = None) -> bool:
        """Set key-value pair with optional expiration"""
        try:
            await self.redis_client.set(key, value, ex=ex)
            return True
        except Exception as e:
            logger.error(f"Error setting key {key}: {e}")
            return False

    async def delete(self, key: str) -> bool:
        """Delete key"""
        try:
            result = await self.redis_client.delete(key)
            return result > 0
        except Exception as e:
            logger.error(f"Error deleting key {key}: {e}")
            return False

    async def exists(self, key: str) -> bool:
        """Check if key exists"""
        try:
            result = await self.redis_client.exists(key)
            return result > 0
        except Exception as e:
            logger.error(f"Error checking key existence {key}: {e}")
            return False

    # JSON operations
    async def set_json(self, key: str, value: Dict[str, Any], ex: Optional[int] = None) -> bool:
        """Set JSON value"""
        try:
            json_value = json.dumps(value, default=str)
            return await self.set(key, json_value, ex=ex)
        except Exception as e:
            logger.error(f"Error setting JSON key {key}: {e}")
            return False

    async def get_json(self, key: str) -> Optional[Dict[str, Any]]:
        """Get JSON value"""
        try:
            value = await self.get(key)
            if value:
                return json.loads(value)
            return None
        except Exception as e:
            logger.error(f"Error getting JSON key {key}: {e}")
            return None

    # List operations (for queues)
    async def lpush(self, key: str, *values: Any) -> int:
        """Push values to left of list"""
        try:
            return await self.redis_client.lpush(key, *values)
        except Exception as e:
            logger.error(f"Error lpush to {key}: {e}")
            return 0

    async def rpop(self, key: str) -> Optional[str]:
        """Pop value from right of list"""
        try:
            return await self.redis_client.rpop(key)
        except Exception as e:
            logger.error(f"Error rpop from {key}: {e}")
            return None

    async def brpop(self, keys: List[str], timeout: int = 0) -> Optional[tuple]:
        """Blocking pop from right of list"""
        try:
            return await self.redis_client.brpop(keys, timeout=timeout)
        except Exception as e:
            logger.error(f"Error brpop from {keys}: {e}")
            return None

    async def llen(self, key: str) -> int:
        """Get list length"""
        try:
            return await self.redis_client.llen(key)
        except Exception as e:
            logger.error(f"Error getting length of {key}: {e}")
            return 0

    async def lrange(self, key: str, start: int, end: int) -> List[str]:
        """Get list range"""
        try:
            return await self.redis_client.lrange(key, start, end)
        except Exception as e:
            logger.error(f"Error getting range from {key}: {e}")
            return []

    # Set operations
    async def sadd(self, key: str, *values: Any) -> int:
        """Add values to set"""
        try:
            return await self.redis_client.sadd(key, *values)
        except Exception as e:
            logger.error(f"Error sadd to {key}: {e}")
            return 0

    async def srem(self, key: str, *values: Any) -> int:
        """Remove values from set"""
        try:
            return await self.redis_client.srem(key, *values)
        except Exception as e:
            logger.error(f"Error srem from {key}: {e}")
            return 0

    async def smembers(self, key: str) -> set:
        """Get all set members"""
        try:
            return await self.redis_client.smembers(key)
        except Exception as e:
            logger.error(f"Error getting members of {key}: {e}")
            return set()

    async def scard(self, key: str) -> int:
        """Get set cardinality"""
        try:
            return await self.redis_client.scard(key)
        except Exception as e:
            logger.error(f"Error getting cardinality of {key}: {e}")
            return 0

    # Hash operations
    async def hset(self, name: str, mapping: Dict[str, Any]) -> int:
        """Set hash fields"""
        try:
            return await self.redis_client.hset(name, mapping=mapping)
        except Exception as e:
            logger.error(f"Error hset to {name}: {e}")
            return 0

    async def hget(self, name: str, key: str) -> Optional[str]:
        """Get hash field"""
        try:
            return await self.redis_client.hget(name, key)
        except Exception as e:
            logger.error(f"Error hget from {name}.{key}: {e}")
            return None

    async def hgetall(self, name: str) -> Dict[str, str]:
        """Get all hash fields"""
        try:
            return await self.redis_client.hgetall(name)
        except Exception as e:
            logger.error(f"Error hgetall from {name}: {e}")
            return {}

    async def hdel(self, name: str, *keys: str) -> int:
        """Delete hash fields"""
        try:
            return await self.redis_client.hdel(name, *keys)
        except Exception as e:
            logger.error(f"Error hdel from {name}: {e}")
            return 0

    # Pub/Sub operations
    async def publish(self, channel: str, message: Any) -> int:
        """Publish message to channel"""
        try:
            if isinstance(message, dict):
                message = json.dumps(message, default=str)
            return await self.redis_client.publish(channel, message)
        except Exception as e:
            logger.error(f"Error publishing to {channel}: {e}")
            return 0

    async def subscribe_to_channels(self, channels: List[str], callback):
        """Subscribe to multiple channels with callback"""
        try:
            self.pubsub = self.redis_client.pubsub()
            await self.pubsub.subscribe(*channels)
            
            async for message in self.pubsub.listen():
                if message['type'] == 'message':
                    try:
                        await callback(message['channel'], message['data'])
                    except Exception as e:
                        logger.error(f"Error in subscription callback: {e}")
                        
        except Exception as e:
            logger.error(f"Error in subscription: {e}")

    async def unsubscribe_from_channels(self, channels: List[str]):
        """Unsubscribe from channels"""
        try:
            if self.pubsub:
                await self.pubsub.unsubscribe(*channels)
        except Exception as e:
            logger.error(f"Error unsubscribing: {e}")

    # Job-specific operations
    async def create_job_status(self, job_id: str, status_data: Dict[str, Any], ttl: int = 3600):
        """Create job status entry"""
        return await self.set_json(f"job_status:{job_id}", status_data, ex=ttl)

    async def update_job_status(self, job_id: str, updates: Dict[str, Any]):
        """Update job status"""
        current_status = await self.get_json(f"job_status:{job_id}")
        if current_status:
            current_status.update(updates)
            return await self.set_json(f"job_status:{job_id}", current_status, ex=3600)
        return False

    async def get_job_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        """Get job status"""
        return await self.get_json(f"job_status:{job_id}")

    async def publish_job_progress(self, job_id: str, progress_data: Dict[str, Any]):
        """Publish job progress update"""
        message = {
            "type": "progress",
            "job_id": job_id,
            "data": progress_data,
            "timestamp": progress_data.get("updated_at")
        }
        await self.publish(f"progress:{job_id}", message)

    async def publish_job_result(self, job_id: str, result_data: Dict[str, Any]):
        """Publish job completion result"""
        message = {
            "type": "result",
            "job_id": job_id,
            "data": result_data,
            "timestamp": result_data.get("updated_at")
        }
        await self.publish(f"results:{job_id}", message)

    async def publish_job_error(self, job_id: str, error_data: Dict[str, Any]):
        """Publish job error"""
        message = {
            "type": "error",
            "job_id": job_id,
            "data": error_data,
            "timestamp": error_data.get("updated_at")
        }
        await self.publish(f"errors:{job_id}", message)
```

### Queue Service (app/services/queue_service.py)

```python
import asyncio
import json
import logging
from datetime import datetime
from typing import Optional, Dict, Any, List
import uuid

from .redis_service import RedisService
from ..models.schemas import QueuedJob, JobStatusEnum, ProcessingUpdate

logger = logging.getLogger(__name__)

class QueueService:
    def __init__(self, redis_service: RedisService):
        self.redis = redis_service
        self.job_processor_task = None
        self.is_processing = False
        
        # Queue names
        self.PENDING_QUEUE = "jobs:pending"
        self.PROCESSING_SET = "jobs:processing"
        self.FAILED_QUEUE = "jobs:failed"
        self.COMPLETED_SET = "jobs:completed"
        
        # Processing limits
        self.MAX_CONCURRENT_JOBS = int(os.getenv("MAX_CONCURRENT_JOBS", "5"))
        self.JOB_TIMEOUT = int(os.getenv("JOB_TIMEOUT", "1800"))  # 30 minutes

    async def enqueue_job(self, job_data: QueuedJob) -> bool:
        """Add job to processing queue"""
        try:
            job_json = json.dumps(job_data.dict(), default=str)
            
            # Add to pending queue
            await self.redis.lpush(self.PENDING_QUEUE, job_json)
            
            # Create initial job status
            initial_status = {
                "job_id": job_data.job_id,
                "status": JobStatusEnum.PENDING,
                "progress": 0.0,
                "current_task": "Job queued for processing",
                "created_at": job_data.created_at.isoformat(),
                "updated_at": datetime.now().isoformat(),
                "file_name": os.path.basename(job_data.file_path),
                "file_size": os.path.getsize(job_data.file_path) if os.path.exists(job_data.file_path) else 0
            }
            
            await self.redis.create_job_status(job_data.job_id, initial_status)
            
            logger.info(f"Job {job_data.job_id} enqueued successfully")
            return True
            
        except Exception as e:
            logger.error(f"Error enqueuing job {job_data.job_id}: {e}")
            return False

    async def dequeue_job(self) -> Optional[QueuedJob]:
        """Get next job from queue"""
        try:
            # Check if we're at processing limit
            processing_count = await self.redis.scard(self.PROCESSING_SET)
            if processing_count >= self.MAX_CONCURRENT_JOBS:
                return None
            
            # Get job from queue with timeout
            result = await self.redis.brpop([self.PENDING_QUEUE], timeout=5)
            if not result:
                return None
                
            queue_name, job_data = result
            job_dict = json.loads(job_data)
            job = QueuedJob(**job_dict)
            
            # Move to processing set
            await self.redis.sadd(self.PROCESSING_SET, job.job_id)
            
            # Update job status
            await self.redis.update_job_status(job.job_id, {
                "status": JobStatusEnum.PROCESSING,
                "updated_at": datetime.now().isoformat()
            })
            
            logger.info(f"Job {job.job_id} dequeued for processing")
            return job
            
        except Exception as e:
            logger.error(f"Error dequeuing job: {e}")
            return None

    async def complete_job(self, job_id: str, results: Optional[Dict[str, Any]] = None):
        """Mark job as completed"""
        try:
            # Remove from processing set
            await self.redis.srem(self.PROCESSING_SET, job_id)
            
            # Add to completed set
            await self.redis.sadd(self.COMPLETED_SET, job_id)
            
            # Update job status
            final_status = {
                "status": JobStatusEnum.COMPLETED,
                "progress": 100.0,
                "current_task": "Processing completed successfully",
                "updated_at": datetime.now().isoformat()
            }
            
            if results:
                final_status["results"] = results
            
            await self.redis.update_job_status(job_id, final_status)
            
            # Publish completion
            job_status = await self.redis.get_job_status(job_id)
            await self.redis.publish_job_result(job_id, job_status)
            
            logger.info(f"Job {job_id} completed successfully")
            
        except Exception as e:
            logger.error(f"Error completing job {job_id}: {e}")

    async def fail_job(self, job_id: str, error_message: str):
        """Mark job as failed"""
        try:
            # Remove from processing set
            await self.redis.srem(self.PROCESSING_SET, job_id)
            
            # Add to failed queue for potential retry
            failed_job_data = {
                "job_id": job_id,
                "error": error_message,
                "failed_at": datetime.now().isoformat(),
                "retry_count": 0
            }
            await self.redis.lpush(self.FAILED_QUEUE, json.dumps(failed_job_data))
            
            # Update job status
            await self.redis.update_job_status(job_id, {
                "status": JobStatusEnum.FAILED,
                "error": error_message,
                "updated_at": datetime.now().isoformat()
            })
            
            # Publish error
            job_status = await self.redis.get_job_status(job_id)
            await self.redis.publish_job_error(job_id, job_status)
            
            logger.error(f"Job {job_id} failed: {error_message}")
            
        except Exception as e:
            logger.error(f"Error failing job {job_id}: {e}")

    async def update_job_progress(self, job_id: str, progress: float, current_task: str, **kwargs):
        """Update job progress"""
        try:
            updates = {
                "progress": min(100.0, max(0.0, progress)),
                "current_task": current_task,
                "updated_at": datetime.now().isoformat()
            }
            updates.update(kwargs)
            
            await self.redis.update_job_status(job_id, updates)
            
            # Publish progress update
            job_status = await self.redis.get_job_status(job_id)
            await self.redis.publish_job_progress(job_id, job_status)
            
            logger.debug(f"Job {job_id} progress updated: {progress}% - {current_task}")
            
        except Exception as e:
            logger.error(f"Error updating job progress {job_id}: {e}")

    async def get_job_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        """Get current job status"""
        return await self.redis.get_job_status(job_id)

    async def get_queue_stats(self) -> Dict[str, int]:
        """Get queue statistics"""
        try:
            stats = {
                "pending": await self.redis.llen(self.PENDING_QUEUE),
                "processing": await self.redis.scard(self.PROCESSING_SET),
                "completed": await self.redis.scard(self.COMPLETED_SET),
                "failed": await self.redis.llen(self.FAILED_QUEUE)
            }
            stats["total"] = sum(stats.values())
            return stats
            
        except Exception as e:
            logger.error(f"Error getting queue stats: {e}")
            return {"pending": 0, "processing": 0, "completed": 0, "failed": 0, "total": 0}

    async def get_queue_size(self) -> int:
        """Get total number of jobs in all queues"""
        stats = await self.get_queue_stats()
        return stats["total"]

    async def cleanup_completed_jobs(self, max_age_hours: int = 24):
        """Clean up old completed jobs"""
        try:
            # Get completed job IDs
            completed_jobs = await self.redis.smembers(self.COMPLETED_SET)
            
            for job_id in completed_jobs:
                job_status = await self.redis.get_job_status(job_id)
                if job_status:
                    updated_at = datetime.fromisoformat(job_status.get("updated_at", ""))
                    age_hours = (datetime.now() - updated_at).total_seconds() / 3600
                    
                    if age_hours > max_age_hours:
                        # Remove from completed set
                        await self.redis.srem(self.COMPLETED_SET, job_id)
                        
                        # Delete job status
                        await self.redis.delete(f"job_status:{job_id}")
                        
                        logger.info(f"Cleaned up completed job {job_id}")
                        
        except Exception as e:
            logger.error(f"Error cleaning up completed jobs: {e}")

    async def start_job_processor(self):
        """Start the background job processor"""
        if self.job_processor_task and not self.job_processor_task.done():
            logger.warning("Job processor is already running")
            return
            
        self.is_processing = True
        self.job_processor_task = asyncio.create_task(self._job_processor_loop())
        logger.info("Job processor started")

    async def stop_job_processor(self):
        """Stop the background job processor"""
        self.is_processing = False
        
        if self.job_processor_task:
            self.job_processor_task.cancel()
            try:
                await self.job_processor_task
            except asyncio.CancelledError:
                pass
                
        logger.info("Job processor stopped")

    async def _job_processor_loop(self):
        """Main job processing loop"""
        while self.is_processing:
            try:
                # Get next job
                job = await self.dequeue_job()
                if not job:
                    # No jobs available, wait a bit
                    await asyncio.sleep(1)
                    continue
                
                # Process job in background task
                asyncio.create_task(self._process_job(job))
                
            except Exception as e:
                logger.error(f"Error in job processor loop: {e}")
                await asyncio.sleep(5)  # Wait before retrying

    async def _process_job(self, job: QueuedJob):
        """Process a single job"""
        try:
            # Import here to avoid circular imports
            from .extraction_service import ExtractionService
            
            extraction_service = ExtractionService(self.redis, self)
            await extraction_service.process_document(job)
            
        except Exception as e:
            logger.error(f"Error processing job {job.job_id}: {e}")
            await self.fail_job(job.job_id, str(e))

    async def retry_failed_jobs(self, max_retries: int = 3):
        """Retry failed jobs"""
        try:
            # Get failed jobs
            failed_jobs_data = await self.redis.lrange(self.FAILED_QUEUE, 0, -1)
            
            for job_data in failed_jobs_data:
                failed_info = json.loads(job_data)
                retry_count = failed_info.get("retry_count", 0)
                
                if retry_count < max_retries:
                    job_id = failed_info["job_id"]
                    
                    # Remove from failed queue
                    await self.redis.rpop(self.FAILED_QUEUE)
                    
                    # Update retry count and re-queue
                    failed_info["retry_count"] = retry_count + 1
                    await self.redis.lpush(self.FAILED_QUEUE, json.dumps(failed_info))
                    
                    # Reset job status to pending
                    await self.redis.update_job_status(job_id, {
                        "status": JobStatusEnum.PENDING,
                        "progress": 0.0,
                        "current_task": f"Retrying... (attempt {retry_count + 1})",
                        "updated_at": datetime.now().isoformat()
                    })
                    
                    logger.info(f"Retrying job {job_id} (attempt {retry_count + 1})")
                    
        except Exception as e:
            logger.error(f"Error retrying failed jobs: {e}")
```