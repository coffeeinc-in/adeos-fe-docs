# Annotation Processing - Backend

This document details the V2 backend flow for processing annotation extraction jobs using Celery workers, Redis queues, and Pusher notifications.

## Process Overview

The backend receives annotation processing requests, creates batch jobs in MongoDB, delegates processing to Celery workers (up to 8 parallel workers based on CPU count), tracks progress in Redis, processes annotations using AI models in parallel, stores results in Azure Blob, and sends real-time updates via Pusher. Celery automatically handles task queuing, parallel execution, and load distribution across workers.

## Complete Flow Diagram

```mermaid
graph TD
    A[Frontend: POST /process-annotation] --> B[Validate Request]
    B --> C[Create Batch Job in MongoDB]
    C --> D[Initialize Redis Progress Tracker]

    D --> E[Queue Celery Tasks]
    E --> F[Celery: Auto-distribute to Workers]

    F --> G1[Worker 1: Process Annotation 1]
    F --> G2[Worker 2: Process Annotation 2]
    F --> G3[Worker 3-8: Process Others in Parallel]

    C --> H[Return batch_id to Frontend]
    H --> I[Frontend: Subscribe to Pusher Channel]

    G1 --> J1[Fetch Page Image from Azure]
    G2 --> J2[Fetch Page Image from Azure]
    G3 --> J3[Fetch Page Images from Azure]

    J1 --> K1[Crop to BBox]
    J2 --> K2[Crop to BBox]
    J3 --> K3[Crop to BBox]

    K1 --> L1[Pusher: extraction_started]
    K2 --> L2[Pusher: extraction_started]
    K3 --> L3[Pusher: extraction_started]

    L1 --> M1[Run Layout Detection]
    L2 --> M2[Run Layout Detection]
    L3 --> M3[Run Layout Detection]

    M1 --> N1[Pusher: layout_detected]
    M2 --> N2[Pusher: layout_detected]
    M3 --> N3[Pusher: layout_detected]

    N1 --> O1[Process Chunks with AI Models]
    N2 --> O2[Process Chunks with AI Models]
    N3 --> O3[Process Chunks with AI Models]

    O1 --> P1[Pusher: chunk_completed per chunk]
    O2 --> P2[Pusher: chunk_completed per chunk]
    O3 --> P3[Pusher: chunk_completed per chunk]

    P1 --> Q1[Save to MongoDB + Azure Blob]
    P2 --> Q2[Save to MongoDB + Azure Blob]
    P3 --> Q3[Save to MongoDB + Azure Blob]

    Q1 --> R1[Update Redis Progress]
    Q2 --> R2[Update Redis Progress]
    Q3 --> R3[Update Redis Progress]

    R1 --> S1[Pusher: batch_item_completed]
    R2 --> S2[Pusher: batch_item_completed]
    R3 --> S3[Pusher: batch_item_completed]

    S1 --> T[Check All Annotations Complete]
    S2 --> T
    S3 --> T

    T --> U[Update Batch Status: completed]
    U --> V[Remove Batch from Redis]
    V --> W[Pusher: batch_job_completed]
    W --> X[Frontend: Fetch Final Results]

    style A fill:#2563eb,stroke:#1e40af,stroke-width:2px,color:#fff
    style C fill:#dc2626,stroke:#b91c1c,stroke-width:2px,color:#fff
    style D fill:#dc2626,stroke:#b91c1c,stroke-width:2px,color:#fff
    style E fill:#f59e0b,stroke:#d97706,stroke-width:2px,color:#fff
    style F fill:#f59e0b,stroke:#d97706,stroke-width:2px,color:#fff
    style G1 fill:#8b5cf6,stroke:#7c3aed,stroke-width:2px,color:#fff
    style G2 fill:#8b5cf6,stroke:#7c3aed,stroke-width:2px,color:#fff
    style G3 fill:#8b5cf6,stroke:#7c3aed,stroke-width:2px,color:#fff
    style J1 fill:#dc2626,stroke:#b91c1c,stroke-width:2px,color:#fff
    style J2 fill:#dc2626,stroke:#b91c1c,stroke-width:2px,color:#fff
    style J3 fill:#dc2626,stroke:#b91c1c,stroke-width:2px,color:#fff
    style O1 fill:#8b5cf6,stroke:#7c3aed,stroke-width:2px,color:#fff
    style O2 fill:#8b5cf6,stroke:#7c3aed,stroke-width:2px,color:#fff
    style O3 fill:#8b5cf6,stroke:#7c3aed,stroke-width:2px,color:#fff
    style Q1 fill:#dc2626,stroke:#b91c1c,stroke-width:2px,color:#fff
    style Q2 fill:#dc2626,stroke:#b91c1c,stroke-width:2px,color:#fff
    style Q3 fill:#dc2626,stroke:#b91c1c,stroke-width:2px,color:#fff
    style R1 fill:#dc2626,stroke:#b91c1c,stroke-width:2px,color:#fff
    style R2 fill:#dc2626,stroke:#b91c1c,stroke-width:2px,color:#fff
    style R3 fill:#dc2626,stroke:#b91c1c,stroke-width:2px,color:#fff
    style U fill:#dc2626,stroke:#b91c1c,stroke-width:2px,color:#fff
    style V fill:#dc2626,stroke:#b91c1c,stroke-width:2px,color:#fff
    style W fill:#8b5cf6,stroke:#7c3aed,stroke-width:2px,color:#fff
    style X fill:#16a34a,stroke:#15803d,stroke-width:2px,color:#fff
```

## API Endpoint

### POST /process-annotation

Initiates annotation extraction processing.

**Headers**:
```http
Authorization: Bearer {jwt_token}
Content-Type: application/json
```

**Request Body**:
```json
{
  "user_id": "user_12345",
  "file_id": "67890abc",
  "pages": [
    {
      "page_no": 1,
      "annotations": [
        {
          "ann_id": "ann_1",
          "ann_type": "text",
          "bbox": [100, 200, 300, 250]
        },
        {
          "ann_id": "ann_2",
          "ann_type": "table",
          "bbox": [50, 300, 500, 600]
        }
      ]
    },
    {
      "page_no": 2,
      "annotations": [
        {
          "ann_id": "ann_3",
          "ann_type": "diagram",
          "bbox": [150, 100, 450, 400]
        }
      ]
    }
  ]
}
```

**Response**:
```json
{
  "batch_id": "batch_123"
}
```

**Response Time**: Less than 100ms (async task initiation)

## Architecture Highlights

import { Callout } from 'nextra/components'

<Callout type="info">
**Key Architecture Benefits**:

1. **Parallel Processing**: Up to 8 annotations processed simultaneously using Celery workers
2. **Auto-scaling**: Celery automatically distributes tasks across available workers
3. **Thread-safe Progress Tracking**: Redis atomic operations (`HINCRBY`) prevent race conditions
4. **Automatic Cleanup**: Batch removed from Redis when all annotations complete
5. **Real-time Updates**: Pusher notifications keep frontend synchronized
6. **No Manual Coordination**: Celery handles queuing, distribution, and load balancing
</Callout>

## Implementation Flow

<Steps>

### Receive and Validate Request

Parse request payload and validate data.

```python
from fastapi import APIRouter, Depends
from pydantic import BaseModel

router = APIRouter()

class AnnotationItem(BaseModel):
    ann_id: str
    ann_type: str  # "text" | "table" | "diagram" | "field"
    bbox: list[int]  # [x1, y1, x2, y2]

class PageAnnotations(BaseModel):
    page_no: int
    annotations: list[AnnotationItem]

class ProcessRequest(BaseModel):
    user_id: str
    file_id: str
    pages: list[PageAnnotations]

@router.post("/process-annotation")
async def initiate_processing(
    request: ProcessRequest,
    user: dict = Depends(verify_token)
):
    # Validate file exists
    file_doc = await files_collection.find_one({"_id": ObjectId(request.file_id)})
    if not file_doc:
        raise HTTPException(status_code=404, detail="File not found")

    # Create batch job
    batch_id = await create_batch_job(request)

    return {"batch_id": batch_id}
```

### Create Batch Job in MongoDB

Create a job document in jobprocess_collection and queue tasks to Celery.

```python
from datetime import datetime
from celery import group

async def create_batch_job(request: ProcessRequest) -> str:
    # Flatten annotations from all pages
    all_annotations = []
    for page in request.pages:
        for ann in page.annotations:
            all_annotations.append({
                "ann_id": ObjectId(ann.ann_id),
                "type": ann.ann_type,
                "page_no": page.page_no,
                "bbox": ann.bbox,
                "status": "queued",
                "data": None
            })

    # Create job document
    job_doc = {
        "file_id": ObjectId(request.file_id),
        "user_id": request.user_id,
        "status": "queued",
        "createdAt": datetime.utcnow(),
        "annotations": all_annotations,
        "total": len(all_annotations),
        "completed": 0
    }

    # Insert into MongoDB
    result = await jobprocess_collection.insert_one(job_doc)
    batch_id = str(result.inserted_id)

    # Initialize Redis progress tracker
    progress_key = f"batch:progress:{batch_id}"
    redis_client.hset(progress_key, mapping={
        "completed": 0,
        "total": len(all_annotations),
        "status": "queued"
    })
    redis_client.expire(progress_key, 7200)  # 2 hours TTL

    # Queue Celery tasks (one task per annotation)
    # Celery will automatically distribute across available workers
    tasks = group([
        process_annotation.s(batch_id, str(ann["ann_id"]))
        for ann in all_annotations
    ])
    tasks.apply_async()

    return batch_id
```

<Callout type="info">
**Celery Parallel Processing**: Using `group()`, all annotation tasks are queued simultaneously. Celery automatically distributes them across available workers (up to 8 workers based on CPU count). Each worker processes annotations independently and in parallel.
</Callout>

### Background Extraction Worker

Celery task that processes a single annotation. Multiple workers process different annotations in parallel.

```python
from celery import Celery
from pusher import Pusher
import redis
from azure.storage.blob import BlobServiceClient
import openai

celery_app = Celery('tasks', broker='redis://localhost:6379/0')

# Configure Celery to use up to 8 workers (based on CPU count)
celery_app.conf.update(
    worker_concurrency=8,  # Number of parallel workers
    task_acks_late=True,
    worker_prefetch_multiplier=1
)

pusher_client = Pusher(...)
redis_client = redis.Redis(host='localhost', port=6379, db=0)
blob_service_client = BlobServiceClient(...)

@celery_app.task
def process_annotation(batch_id: str, ann_id: str):
    """
    Process a single annotation.
    This task runs in parallel across multiple Celery workers.
    """
    # Fetch batch job
    job_doc = jobprocess_collection.find_one({"_id": ObjectId(batch_id)})
    user_id = job_doc["user_id"]
    file_id = job_doc["file_id"]

    # Find the specific annotation to process
    annotation = None
    for ann in job_doc["annotations"]:
        if str(ann["ann_id"]) == ann_id:
            annotation = ann
            break

    if not annotation:
        raise Exception(f"Annotation {ann_id} not found in batch {batch_id}")

    ann_type = annotation["type"]
    page_no = annotation["page_no"]
    bbox = annotation["bbox"]

    try:
        # Update status to processing
        jobprocess_collection.update_one(
            {"_id": ObjectId(batch_id), "annotations.ann_id": ObjectId(ann_id)},
            {"$set": {"annotations.$.status": "processing"}}
        )

        # Update batch status to processing (if first annotation)
        jobprocess_collection.update_one(
            {"_id": ObjectId(batch_id), "status": "queued"},
            {"$set": {"status": "processing"}}
        )

        # Fetch page image from Azure Blob
        page_doc = pages_collection.find_one({
            "file_id": ObjectId(file_id),
            "page_no": page_no
        })
        page_image_url = page_doc["url"]

        # Download image
        blob_client = blob_service_client.get_blob_client_from_url(page_image_url)
        image_bytes = blob_client.download_blob().readall()

        # Crop to bounding box
        cropped_image = crop_image(image_bytes, bbox)

        # Send extraction started notification
        pusher_client.trigger(user_id, 'extraction_started', {
            'batch_id': batch_id,
            'ann_id': ann_id
        })

        # Run layout detection
        layout_chunks = run_layout_detection(cropped_image)
        # Returns: [{"type": "text", "bbox": [...], "content": bytes}, ...]

        # Send layout detected notification
        pusher_client.trigger(user_id, 'layout_detected', {
            'batch_id': batch_id,
            'ann_id': ann_id,
            'chunk_count': len(layout_chunks)
        })

        # Process each chunk
        chunk_results = []
        for chunk_idx, chunk in enumerate(layout_chunks):
            chunk_type = chunk["type"]
            chunk_image = chunk["content"]

            # Extract based on chunk type
            if chunk_type == "text":
                chunk_data = extract_text_with_gpt(chunk_image)
            elif chunk_type == "table":
                chunk_data = extract_table(chunk_image)
            elif chunk_type == "diagram":
                chunk_data = analyze_diagram(chunk_image)

            chunk_results.append({
                "chunk_index": chunk_idx,
                "type": chunk_type,
                "data": chunk_data
            })

            # Send chunk completed notification
            pusher_client.trigger(user_id, 'chunk_completed', {
                'batch_id': batch_id,
                'ann_id': ann_id,
                'chunk_index': chunk_idx,
                'total_chunks': len(layout_chunks),
                'progress': round(((chunk_idx + 1) / len(layout_chunks)) * 100)
            })

        # Combine all chunk data
        extracted_data = combine_chunk_data(chunk_results, ann_type)

        # Upload processed image to blob
        processed_blob_name = f"processed/{batch_id}/{ann_id}.png"
        processed_blob_client = blob_service_client.get_blob_client(
            container="annotations",
            blob=processed_blob_name
        )
        processed_blob_client.upload_blob(cropped_image, overwrite=True)
        processed_url = processed_blob_client.url

        # Add URL to extracted data
        extracted_data["processed_image_url"] = processed_url

        # Update MongoDB: jobprocess_collection
        jobprocess_collection.update_one(
            {"_id": ObjectId(batch_id), "annotations.ann_id": ObjectId(ann_id)},
            {"$set": {
                "annotations.$.status": "completed",
                "annotations.$.data": extracted_data
            }}
        )

        # Update MongoDB: annotations_collection
        annotations_collection.update_one(
            {"_id": ObjectId(ann_id)},
            {"$set": {
                "status": "completed",
                "data": extracted_data,
                "updated_at": datetime.utcnow()
            }}
        )

        # Atomically increment completed count in Redis
        progress_key = f"batch:progress:{batch_id}"
        completed_count = redis_client.hincrby(progress_key, "completed", 1)

        # Send Pusher notification
        pusher_client.trigger(user_id, 'batch_item_completed', {
            'batch_id': batch_id,
            'ann_id': ann_id,
            'status': 'completed'
        })

        # Check if all annotations are completed
        total_count = int(redis_client.hget(progress_key, "total"))
        if completed_count >= total_count:
            # All annotations completed - mark batch as done
            jobprocess_collection.update_one(
                {"_id": ObjectId(batch_id)},
                {"$set": {"status": "completed"}}
            )

            # Remove batch from Redis
            redis_client.delete(progress_key)

            # Send batch completion notification
            pusher_client.trigger(user_id, 'batch_job_completed', {
                'batch_id': batch_id,
                'status': 'completed',
                'total_processed': total_count
            })

    except Exception as e:
        # Mark annotation as failed
        jobprocess_collection.update_one(
            {"_id": ObjectId(batch_id), "annotations.ann_id": ObjectId(ann_id)},
            {"$set": {
                "annotations.$.status": "failed",
                "annotations.$.error": str(e)
            }}
        )

        # Still increment completed count (failed counts as completed)
        progress_key = f"batch:progress:{batch_id}"
        completed_count = redis_client.hincrby(progress_key, "completed", 1)

        pusher_client.trigger(user_id, 'batch_item_failed', {
            'batch_id': batch_id,
            'ann_id': ann_id,
            'error': str(e)
        })

        # Check if all annotations are processed (including failures)
        total_count = int(redis_client.hget(progress_key, "total"))
        if completed_count >= total_count:
            jobprocess_collection.update_one(
                {"_id": ObjectId(batch_id)},
                {"$set": {"status": "completed"}}
            )

            # Remove batch from Redis
            redis_client.delete(progress_key)

            pusher_client.trigger(user_id, 'batch_job_completed', {
                'batch_id': batch_id,
                'status': 'completed',
                'total_processed': total_count
            })
```

<Callout type="info">
**Parallel Processing Architecture**: Each annotation is processed by a separate Celery task. With 8 workers, up to 8 annotations can be processed simultaneously. Workers use atomic Redis operations (`HINCRBY`) to safely track progress. When the last worker completes, it detects completion and removes the batch from Redis.
</Callout>

### Layout Detection and Chunking

Detect layout elements within the cropped annotation region.

```python
from PIL import Image
import io
import numpy as np
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg

def run_layout_detection(image_bytes: bytes) -> list[dict]:
    """
    Detects layout elements (text blocks, tables, diagrams) in the image.
    Returns a list of chunks with their type and cropped content.
    """
    # Load image
    image = Image.open(io.BytesIO(image_bytes))
    img_array = np.array(image)

    # Initialize layout detection model (Detectron2 or LayoutLM)
    cfg = get_cfg()
    cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml"))
    cfg.MODEL.WEIGHTS = "path/to/layout_detection_weights.pth"
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5
    predictor = DefaultPredictor(cfg)

    # Run detection
    outputs = predictor(img_array)

    # Extract detected regions
    chunks = []
    predictions = outputs["instances"].to("cpu")
    boxes = predictions.pred_boxes
    classes = predictions.pred_classes
    scores = predictions.scores

    # Class mapping: 0=text, 1=table, 2=diagram, 3=figure
    class_map = {0: "text", 1: "table", 2: "diagram", 3: "figure"}

    for idx, (box, cls, score) in enumerate(zip(boxes, classes, scores)):
        x1, y1, x2, y2 = box.tolist()

        # Crop chunk from image
        chunk_image = image.crop((x1, y1, x2, y2))
        chunk_bytes = io.BytesIO()
        chunk_image.save(chunk_bytes, format='PNG')
        chunk_bytes.seek(0)

        chunks.append({
            "type": class_map.get(int(cls), "text"),
            "bbox": [int(x1), int(y1), int(x2), int(y2)],
            "content": chunk_bytes.read(),
            "confidence": float(score)
        })

    # If no chunks detected, treat entire image as single text chunk
    if not chunks:
        img_bytes = io.BytesIO()
        image.save(img_bytes, format='PNG')
        img_bytes.seek(0)

        chunks = [{
            "type": "text",
            "bbox": [0, 0, image.width, image.height],
            "content": img_bytes.read(),
            "confidence": 1.0
        }]

    return chunks

def combine_chunk_data(chunk_results: list[dict], ann_type: str) -> dict:
    """
    Combines extracted data from all chunks into final result.
    """
    if ann_type == "text":
        # Concatenate all text chunks
        combined_text = " ".join([
            chunk["data"]["extracted_text"]
            for chunk in chunk_results
            if chunk["type"] == "text"
        ])

        return {
            "extracted_text": combined_text,
            "chunks": chunk_results,
            "chunk_count": len(chunk_results)
        }

    elif ann_type == "table":
        # Combine table data
        all_tables = [
            chunk["data"]
            for chunk in chunk_results
            if chunk["type"] == "table"
        ]

        return {
            "tables": all_tables,
            "table_count": len(all_tables),
            "chunks": chunk_results
        }

    elif ann_type == "diagram":
        # Combine diagram analysis
        descriptions = [
            chunk["data"]["description"]
            for chunk in chunk_results
            if chunk["type"] == "diagram"
        ]

        return {
            "description": " ".join(descriptions),
            "elements": [
                elem
                for chunk in chunk_results
                if chunk["type"] == "diagram"
                for elem in chunk["data"].get("elements", [])
            ],
            "chunks": chunk_results
        }

    # Default: return all chunks
    return {
        "chunks": chunk_results,
        "chunk_count": len(chunk_results)
    }
```

<Callout type="info">
**Layout Detection**: Using Detectron2 or LayoutLMv3, the system identifies different content types within a single annotation region. This allows for specialized processing of text blocks, tables, and diagrams even when they appear together in one annotation.
</Callout>

### Extraction Functions

AI-powered extraction based on chunk type.

```python
import openai
from PIL import Image
import io
import base64

def extract_text_with_gpt(image_bytes: bytes) -> dict:
    # Convert to base64
    image_b64 = base64.b64encode(image_bytes).decode()

    # Call GPT-4 Vision
    response = openai.ChatCompletion.create(
        model="gpt-4-vision-preview",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "Extract all text from this image."},
                    {"type": "image_url", "image_url": f"data:image/png;base64,{image_b64}"}
                ]
            }
        ],
        max_tokens=1000
    )

    extracted_text = response.choices[0].message.content

    return {
        "extracted_text": extracted_text,
        "confidence": 0.95,
        "model": "gpt-4-vision"
    }

def extract_table(image_bytes: bytes) -> dict:
    # Use table extraction model
    # Return structured table data
    return {
        "table_data": [
            ["Header 1", "Header 2"],
            ["Cell 1", "Cell 2"]
        ],
        "rows": 2,
        "columns": 2,
        "confidence": 0.92
    }

def analyze_diagram(image_bytes: bytes) -> dict:
    # Use diagram analysis model
    return {
        "description": "Flowchart showing process steps...",
        "elements": ["start", "process", "decision", "end"],
        "confidence": 0.88
    }

def extract_field(image_bytes: bytes) -> dict:
    # Extract single field value
    return {
        "field_value": "John Doe",
        "field_type": "name",
        "confidence": 0.97
    }
```

</Steps>

## Database Schema

### jobprocess_collection

```javascript
{
  _id: ObjectId("batch_123..."),  // This is the batch_id
  file_id: ObjectId("67890abc..."),
  user_id: "user_12345",
  status: "processing",  // "queued" | "processing" | "completed" | "failed"
  createdAt: ISODate("2025-10-03T10:05:00Z"),
  annotations: [
    {
      ann_id: ObjectId("ann_1..."),
      type: "text",
      page_no: 1,
      bbox: [100, 200, 300, 250],
      status: "completed",  // "queued" | "processing" | "completed" | "failed"
      data: {
        extracted_text: "Sample text...",
        confidence: 0.98,
        processed_image_url: "https://..."
      },
      error: null
    },
    {
      ann_id: ObjectId("ann_2..."),
      type: "table",
      page_no: 1,
      bbox: [50, 300, 500, 600],
      status: "processing",
      data: null,
      error: null
    }
  ]
}
```

### annotations_collection

```javascript
{
  _id: ObjectId("ann_1..."),
  file_id: ObjectId("67890abc..."),
  page_no: 1,
  type: "text",
  bbox: [100, 200, 300, 250],
  status: "completed",
  data: {
    extracted_text: "Sample text...",
    confidence: 0.98,
    processed_image_url: "https://adeos.blob.core.windows.net/annotations/processed/batch_123/ann_1.png"
  },
  created_at: ISODate("2025-10-03T10:05:00Z"),
  updated_at: ISODate("2025-10-03T10:06:30Z")
}
```

## Celery Configuration

### Worker Setup

Celery is configured to run up to 8 parallel workers based on CPU count:

```python
# celery_app.py
from celery import Celery

celery_app = Celery(
    'adeos_tasks',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/0'
)

# Configure workers
celery_app.conf.update(
    worker_concurrency=8,              # Number of parallel worker processes
    task_acks_late=True,                # Acknowledge tasks after completion
    worker_prefetch_multiplier=1,       # Fetch one task at a time per worker
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
)
```

### Starting Workers

```bash
# Start Celery worker with 8 concurrent processes
celery -A celery_app worker --loglevel=info --concurrency=8

# In Docker
docker-compose.yml:
  worker:
    command: celery -A celery_app worker --loglevel=info --concurrency=8
    deploy:
      replicas: 1
```

<Callout type="info">
**Auto-scaling**: Celery automatically distributes tasks across available workers. With 8 workers, 8 annotations can be processed simultaneously. Celery manages its own task queue internally using Redis as a message broker.
</Callout>

## Redis Integration

### Progress Tracking

Redis is used ONLY for tracking batch progress, not for task queuing (Celery manages its own queue).

```python
# Initialize progress tracker
Key: batch:progress:batch_123
Type: HASH
Value: {
  "completed": "0",
  "total": "8",
  "status": "processing"
}
TTL: 7200 seconds  # Auto-expire after 2 hours

# Atomically increment completed count
redis_client.hincrby("batch:progress:batch_123", "completed", 1)

# When all complete, delete from Redis
redis_client.delete("batch:progress:batch_123")
```

### Key Features

- **Atomic Operations**: `HINCRBY` ensures thread-safe counter increments across parallel workers
- **TTL**: Auto-expire prevents stale data from accumulating
- **Cleanup**: Batch removed from Redis when all annotations complete
- **Concurrent Access**: Multiple workers can safely update the same batch progress

## Pusher Events

### extraction_started

Sent when annotation extraction begins.

```python
pusher_client.trigger(
    channel=user_id,
    event='extraction_started',
    data={
        'batch_id': 'batch_123',
        'ann_id': 'ann_1'
    }
)
```

### layout_detected

Sent after layout detection identifies chunks.

```python
pusher_client.trigger(
    channel=user_id,
    event='layout_detected',
    data={
        'batch_id': 'batch_123',
        'ann_id': 'ann_1',
        'chunk_count': 3
    }
)
```

### chunk_completed

Sent after each chunk is processed.

```python
pusher_client.trigger(
    channel=user_id,
    event='chunk_completed',
    data={
        'batch_id': 'batch_123',
        'ann_id': 'ann_1',
        'chunk_index': 0,
        'total_chunks': 3,
        'progress': 33  # Percentage
    }
)
```

### batch_item_completed

```python
pusher_client.trigger(
    channel=user_id,
    event='batch_item_completed',
    data={
        'batch_id': 'batch_123',
        'ann_id': 'ann_1',
        'status': 'completed'
    }
)
```

### batch_item_failed

```python
pusher_client.trigger(
    channel=user_id,
    event='batch_item_failed',
    data={
        'batch_id': 'batch_123',
        'ann_id': 'ann_2',
        'error': 'AI model timeout'
    }
)
```

### batch_job_completed

```python
pusher_client.trigger(
    channel=user_id,
    event='batch_job_completed',
    data={
        'batch_id': 'batch_123',
        'status': 'completed',
        'total_processed': 5
    }
)
```

## Error Handling

<Callout type="warning">
Handle AI model failures and blob storage errors gracefully.
</Callout>

### AI Model Timeout

```python
try:
    extracted_data = extract_text_with_gpt(cropped_image)
except openai.error.Timeout:
    # Retry with exponential backoff
    for attempt in range(3):
        time.sleep(2 ** attempt)
        try:
            extracted_data = extract_text_with_gpt(cropped_image)
            break
        except openai.error.Timeout:
            if attempt == 2:
                raise Exception("AI model timeout after 3 attempts")
```

### Blob Upload Failure

```python
try:
    processed_blob_client.upload_blob(cropped_image, overwrite=True)
except Exception as e:
    # Log error but continue processing
    logger.error(f"Blob upload failed for {ann_id}: {e}")
    extracted_data["processed_image_url"] = None
```

## Performance Metrics

| Metric | Value |
|--------|-------|
| POST /process-annotation | Less than 100ms |
| Text Extraction (GPT-4 Vision) | 2-4s per annotation |
| Table Extraction | 5-8s per annotation |
| Diagram Analysis | 6-10s per annotation |
| Blob Upload | 500-1000ms per image |
| Pusher Notification | Less than 100ms |
| Concurrent Workers | Up to 8 (based on CPU count) |
| Parallel Processing | 8 annotations simultaneously |
| Throughput | ~120-240 annotations/minute (with 8 workers) |

### Optimization Strategies

**Celery Handles Parallelization**:
Celery automatically manages parallel processing. Simply configure worker concurrency:

```python
# No need for manual async/await - Celery handles it
celery_app.conf.worker_concurrency = 8

# Queue all tasks at once - Celery distributes automatically
tasks = group([
    process_annotation.s(batch_id, ann_id)
    for ann_id in annotation_ids
])
tasks.apply_async()
```

**Redis Atomic Operations**:
Use atomic operations to prevent race conditions with concurrent workers:

```python
# Thread-safe increment
completed = redis_client.hincrby(f"batch:progress:{batch_id}", "completed", 1)

# Compare-and-set pattern
pipeline = redis_client.pipeline()
pipeline.watch(progress_key)
current = pipeline.hget(progress_key, "completed")
if int(current) + 1 == total:
    pipeline.multi()
    pipeline.delete(progress_key)
    pipeline.execute()
```

**Batch Processing Configuration**:
```bash
# Adjust based on server capacity
celery -A celery_app worker \
  --concurrency=8 \           # Number of parallel workers
  --prefetch-multiplier=1 \   # Fetch one task at a time
  --max-tasks-per-child=100   # Restart worker after 100 tasks (memory management)
```

## Code References

**Process Annotation Endpoint**: `backend/api/routes/batch/process.py:100-140`
**Extraction Worker**: `backend/workers/extraction_worker.py:30-200`
**AI Extractors**: `backend/services/ai_extractors.py:10-150`
**Blob Service**: `backend/services/azure_blob.py:20-80`

## Related Documentation

- [Annotation Processing Frontend](/v2-iteration/annotation-processing-frontend) - Frontend implementation
- [Existing File Backend](/v2-iteration/existing-file-backend) - Loading existing batches
- [File Upload Backend](/v2-iteration/file-upload-backend) - File and page processing
